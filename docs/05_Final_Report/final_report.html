<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>final_report</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>


</head>

<body>

<h1 id="toc_0">Capstone Project Final Report: Quebec Iron Ore 1</h1>

<p>Jingyun Chen, Socorro Dominguez, Milos Milic, Aditya Sharma</p>

<h4 id="toc_1">Executive summary</h4>

<p>Image Region Optimizing Neural Network(IRONN) has been developed for QIO to leverage Machine Learning (ML) techniques in order to improve ore yields while reducing operational costs. We are providing a Python package that contains a trained neural network that categorises Ore, Dilution Waste and Contamination Waste in candidate images to 85% accuracy. We are also providing two dashboard applications that can be used in assessing image quality.</p>

<h4 id="toc_2">Introduction</h4>

<p>After a rock face is blasted, operators are required to obtain details from geologists who demarcate different rock types by going through the images taken by the operators. This information loop is necessary in order to collect appropriate blends of different rock types. However, geologists are not available at all times to help the operators make their decisions, which reduces extraction efficiency.</p>

<p>The available data includes 650 labeled images of blasted rock faces from different days and blast sites. These images were collected by the operators (original images) and then hand-annotated by the geologists (reference images). An example of these images is shown below:</p>

<p><center></p>

<table>
<thead>
<tr>
<th>Original Image</th>
<th>Reference Image</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="../00_images/05_final_report/00_orig.JPG" alt="Figure 1: Original Image" width="320" /></td>
<td><img src="../00_images/05_final_report/00_ref.jpg" alt="Figure 2: Reference Image" width="320" /></td>
</tr>
</tbody>
</table>

<p></center></p>

<p>We have broken down the problem into the following <strong>tangible objectives</strong>:</p>

<p><strong>1. Labeling the original images using the reference images:</strong> Labeling refers to defining the area on an image that defines a specific rock type. This is the most important task in the project as it is a supervised learning problem. A properly labeled dataset is essential to move forward with building a model that can learn to segment different rock types. </p>

<p><strong>2. Data analysis and feature extraction:</strong> Identify image properties that might be useful in differentiating rocks. We identified image properties that might be useful such as colour features of the picture and the camera settings used. This also allows to assess the quality of the original images and improve image collection.</p>

<p><strong>3. Preprocessing images and generating labels for image segmentation:</strong> This step involves reading the labels and converting them into masks that will classify each pixel in the image into a rock class or background.</p>

<p><strong>4. Building a ML model:</strong> Created a model that can learn different properties of rocks and can segment different rock types based on these learned features.</p>

<p><strong>5. Model prediction:</strong> Visualized the model prediction results by superimposing the mask of the predicted labels over the original images. This allows identifying the different rock types on a blasted rock face.</p>

<h4 id="toc_3">Data Science Techniques</h4>

<p>IRONN is a deep learning-based product that segments and classifies different rock types in a blasted rock face image. Using physical characteristics such as rocks’ colour, IRONN can currently segment Ore, Dilution Waste and Contamination Waste with around 85% pixel-wise accuracy.</p>

<p align="center">
    <img src="../00_images/05_final_report/08_pipeline_diagram.png" width="500" />
</p>

<p>The <strong>tangible objectives</strong> were addressed using the following <strong>Data Science techniques</strong>:</p>

<p><strong>1. Labeling using an open source tool:</strong> The original images were relabeled using <a href="https://github.com/wkentaro/labelme">LabelMe</a>. LabelMe allowed us to obtain the coordinates of the polygons along with their class labels, which provided feasible training data. However, it is an open source tool, and it contains some unaddressed software bugs. </p>

<p align="center">
  <img width="350" src="../00_images/05_final_report/01_labeled.jpg" alt="Labeled Example">
</p>

<p>As an improvement, it might be worth considering using a paid labeling tool that will be more robust.</p>

<p><strong>2. Exploratory Data Analysis (EDA) using dashboards:</strong> We went through the images manually and programmatically to find useful features and patterns. We extracted image roughness properties on the three colour channels (RGB) from the images in order to study patterns in different rock types. We analyzed the differences in features through a pairwise ANOVA test with post-hoc Tukey in our imbalanced data set. This was done to check for significant differences between different rock types. We also analyzed camera settings data as the images came from different devices. We wanted to see the variability in how the images were taken and if that would affect the model. </p>

<p>For further EDA we built an interactive <a href="https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503">plotly dashboard</a> named iCA (image Colour Analysis)  that helps identify any outliers in order to improve the quality of the training data. </p>

<p align="center">
    <img src="../00_images/05_final_report/02_plotly_dash.png" alt="Plotly Dash" width="350" style="border:1px solid grey"/>
</p>

<p>To address data quality assurance, we built an <a href="https://shiny.rstudio.com/">R shiny dashboard</a> named iQA (image Quality Assurance) which can unify the concept of &quot;How should an image be taken?&quot;. In this way, better data can be gathered to further develop IRONN.</p>

<p align="center">
    <img src="../00_images/05_final_report/03_shiny_dash.JPG" alt="Shiny Dash" width="350" />
</p>

<p><strong>3. Preprocessing the images using python library <a href="https://opencv.org/">OpenCV</a>:</strong> We read the polygon coordinates and labels to generate masks for each rock type using OpenCV. This created the dataset that can be fed into the model. The dataset specifies the format of the output that will be obtained once the model finishes training and is used for prediction.</p>

<p><center></p>

<table>
<thead>
<tr>
<th>Mask</th>
<th>Superimposed Image</th>
</tr>
</thead>

<tbody>
<tr>
<td><img src="../00_images/05_final_report/04_mask.JPG" width="320"/></td>
<td><img src="../00_images/05_final_report/04_superimposed.jpg" width="320" /></td>
</tr>
</tbody>
</table>

<p></center></p>

<p>There are other libraries that can be used for processing images such as Scikit-Image and PIL. We chose OpenCV because it has been heavily developed and maintained. Other libraries require more care as they handle image reading and processing differently.</p>

<p><strong>4. Building a deep learning model using Convolutional Neural Network (CNN):</strong>
We used a CNN-based architecture called Fully Convolutional Networks (FCN) to build and train a model for the segmentation task. We first checked if our model could segment the blasted rock face from the rest of the background (sky and ground). We extracted the convex hull polygon coordinates that included all polygons (masks) in a single polygon and called this a blasted face. Then, we trained the model on this data to the first segment the blasted face and background.</p>

<p><center></p>

<table>
<thead>
<tr>
<th>Image Name</th>
<th>Image</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>Convex Hull</strong></td>
<td><img src="../00_images/05_final_report/05_ch_poly.jpg" width="320" /></td>
</tr>
<tr>
<td><strong>Mask</strong></td>
<td><img src="../00_images/05_final_report/05_ch_mask.png" width="320" /></td>
</tr>
<tr>
<td><strong>Superimposed Image</strong></td>
<td><img src="../00_images/05_final_report/05_ch_superimposed.png" width="320" /></td>
</tr>
</tbody>
</table>

<p></center>
<br><br><br><br><br><br></p>

<p>We obtained the following results on unseen images:</p>

<p align="center">
    <img src="../00_images/05_final_report/06_ch_pred.png" width="350" style="border:1px solid grey"/>
</p>

<p>Since we deemed it possible to train a model that could segment blasted face from the background, we decided to move to the next step and implement a more complex model. We grouped the 8 different types of rocks into three base categories: &quot;Ore&quot;, &quot;Dilution Waste&quot;, and &quot;Contamination Waste&quot; and modified the code to handle these new classes. We achieved a pixel-wise accuracy of about 85% on the validation set.</p>

<p>Some points to note about this model:</p>

<ul>
<li>It takes over 3 hours to train on 650 images and the training time will increase with more images</li>
<li>It uses transfer learning from the ImageNet dataset which reduces the training time</li>
<li>It uses a standard architecture for image segmentation tasks (FCN). There are other deep learning algorithms such as DeepLabv and Mask-RCNN that might perform better on the task.</li>
<li>The current cost of training the model on an AWS instance with a 12 GB memory GPU is around 10$.</li>
</ul>

<p><strong>5. Prediction and Visualization:</strong>
To assess IRONN’s performance, we removed 50 images from the training set and used them as a validation set for visualizing the model performance. Once trained, we predicted on these images and superimposed the prediction.</p>

<p>For the final visualized output, the label output is superimposed on the original images. This is different from the validation during training which divides the training data into an 80-20 split and uses the 80% to train and 20% to validate using metrics such as pixel-wise accuracy and the intersection over union metric. Once finalized, we will train the model on the complete training set with all labeled images and deliver to QIO for further improvement.</p>

<p>Here is an example of an original image, label mask and model prediction:</p>

<p><br><br><br><br>
<center></p>

<table>
<thead>
<tr>
<th>Image Name</th>
<th>Image</th>
</tr>
</thead>

<tbody>
<tr>
<td><strong>Original Image</strong></td>
<td><img src="../00_images/05_final_report/07_orig_val.jpg" width="320" /></td>
</tr>
<tr>
<td><strong>Superimposed Mask Label</strong></td>
<td><img src="../00_images/05_final_report/07_superimposed_val.jpg" width="320" /></td>
</tr>
<tr>
<td><strong>Superimpose Prediction</strong></td>
<td><img src="../00_images/05_final_report/07_pred_val.jpg" width="320" /></td>
</tr>
</tbody>
</table>

<p></center>
<br><br><br><br><br></p>

<h4 id="toc_4">Data product</h4>

<p>We are <strong>delivering</strong> a data product that consists of the following:</p>

<ul>
<li>a Python package that:

<ul>
<li>reads images and label files as training data</li>
<li>trains an FCN-based model on the training data</li>
<li>predicts and segments different rock types on new images</li>
</ul></li>
</ul>

<p>The package is designed so that the model can be retrained easily with new training data or predict over new images using the currently trained model. </p>

<p>We are also providing the following: 
proper documentation of the code along with clear instructions on how to use the package
thorough documentation of the findings during the EDA, for colour and EXIF data analysis.
one <strong>plotly dashboard</strong> to understand better the colour related features and properties (iCA)
one <strong>R shiny dashboard</strong> for quality assurance and data collection improvement (iQA)</p>

<p>The package has been built with the overall pipeline of the project in mind so that any change in the process (eg. adding new images or new rocks) is easy to incorporate.</p>

<p>Our work has answered the question: &quot;Can ML techniques be used to train a model where the image input data is segmented and then used to predict different rock types?&quot; IRONN has proven that it is possible to predict different classes of rock types, although it needs further development in order to segment rocks with fine-grained differences.</p>

<h4 id="toc_5">Results</h4>

<p>In the confusion matrix below, we see the number of pixels classified correctly and incorrectly for each rock type class. Around 95% of the pixels belonging to the ORE are correctly classified by the model.</p>

<p><strong>Confusion Matrix</strong></p>

<p><center></p>

<table>
<thead>
<tr>
<th style="text-align: center">Predicted</th>
<th style="text-align: center">Background</th>
<th style="text-align: center">Ore</th>
<th style="text-align: center">CW</th>
<th style="text-align: center">DW</th>
<th style="text-align: center">ALL</th>
</tr>
</thead>

<tbody>
<tr>
<td style="text-align: center"><strong>True</strong></td>
<td style="text-align: center"></td>
<td style="text-align: center"></td>
<td style="text-align: center"></td>
<td style="text-align: center"></td>
<td style="text-align: center"></td>
</tr>
<tr>
<td style="text-align: center"><strong>Background</strong></td>
<td style="text-align: center">1,553,000</td>
<td style="text-align: center">43,000</td>
<td style="text-align: center">1,000</td>
<td style="text-align: center">9,000</td>
<td style="text-align: center">1,606,000</td>
</tr>
<tr>
<td style="text-align: center"><strong>Ore</strong></td>
<td style="text-align: center">19,000</td>
<td style="text-align: center">528,000</td>
<td style="text-align: center">500</td>
<td style="text-align: center">11,000</td>
<td style="text-align: center">558,000</td>
</tr>
<tr>
<td style="text-align: center"><strong>CW</strong></td>
<td style="text-align: center">16,000</td>
<td style="text-align: center">171,000</td>
<td style="text-align: center">51,000</td>
<td style="text-align: center">8,000</td>
<td style="text-align: center">246,000</td>
</tr>
<tr>
<td style="text-align: center"><strong>DW</strong></td>
<td style="text-align: center">2,000</td>
<td style="text-align: center">9,000</td>
<td style="text-align: center">00</td>
<td style="text-align: center">87,000</td>
<td style="text-align: center">99,000</td>
</tr>
<tr>
<td style="text-align: center"><strong>ALL</strong></td>
<td style="text-align: center">1,589,000</td>
<td style="text-align: center">751,000</td>
<td style="text-align: center">53,000</td>
<td style="text-align: center">116,000</td>
<td style="text-align: center">2,509,000</td>
</tr>
</tbody>
</table>

<p></center></p>

<p>Currently, this Python Package needs to be installed in a QIO’s Data Scientist&#39;s computer so that results can be reproduced and IRONN can be further developed.</p>

<h5 id="toc_6">Justification over other products/interfaces</h5>

<p>IRONN is a proof of concept and it is not a full-fledged product yet. To our knowledge, there are no software applications that are being used to segment rock types in the mining industry.</p>

<h4 id="toc_7">Conclusions and Recommendations</h4>

<p>IRONN shows that ML architectures are powerful tools that can improve operational costs for QIO. If IRONN is further developed and implemented, mine operators will eventually be able to rely on it to classify rocks and send appropriate blends to the crusher. </p>

<p>In order for IRONN to be used on a daily basis, the following improvements are needed:</p>

<ul>
<li>Gathering more high-quality images and labeling them properly.</li>
<li>Balancing data; some rock types are underrepresented. </li>
<li>Retraining the model using 8 different rock types instead of its group categories.</li>
<li>Building a user-friendly interface that can provide segmentation results in real-time.</li>
</ul>

<p>New images with better quality have been actively collected. To keep the model performance at its current stage with the 8 rock types, we would need at least double the amount of labeled data that we currently possess. We would encourage to continue developing IRONN so that it can eventually be a complete product.</p>




</body>

</html>
